{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "4YiWqpBxencG",
        "lfxaXfLhZAFb",
        "1BDvUqQ8ZuBp"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maestro2496/Machine-Learning/blob/main/BigDataAssessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if this instance of the notebook already has files present\n",
        "# and thus determine which steps required prior to reading in file and handling the data\n",
        "!ls\n"
      ],
      "metadata": {
        "id": "IfUDk8dj-iuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40ZMzK1NEOxG",
        "collapsed": true
      },
      "source": [
        "# set-up spark (NB if Apache amend versions on download site we will need to amend path in wget command)\n",
        "## NOTE that this version would make use of Hadoop if installed BUT that HDFS & Hadoop is not installed on our Colab\n",
        "## (we are only using a single node (probably as a VM) so we will not be able to benefit from parallelism)\n",
        "!clear\n",
        "!echo welcome\n",
        "\n",
        "!rm -f spark-3.4.[01]-bin-hadoop3.tgz*\n",
        "!rm -rf spark-3.4.[01]-bin-hadoop3\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://downloads.apache.org/spark/spark-3.4.2/spark-3.4.2-bin-hadoop3.tgz\n",
        "!tar -xf spark-3.4.2-bin-hadoop3.tgz\n",
        "\n",
        "!ls -alt\n",
        "print(\"standalone Spark is now installed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZguLonUE-js"
      },
      "source": [
        "# init spark (ensure SPARK_HOME set to same version as we download earlier)\n",
        "!pip3 install findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.2-bin-hadoop3\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkConf, SparkContext\n",
        "# the next line gives us 'local' mode. try 'local[2]' to use 2 cores or 'master:NNNN' to run on Spark standalone cluster at port NNNN\n",
        "spark_conf = SparkConf().setMaster('local[2]').setAppName('MyApp')\n",
        "sc = SparkContext(conf=spark_conf)\n",
        "# see what we have by examining the Spark User Interface\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "SparkSession.builder.getOrCreate()\n",
        "##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABQESgtdFZxa"
      },
      "source": [
        "## this is how one could upload a file into colab using the colab GUI (uncomment both lines if want to try it)\n",
        "\n",
        "#from google.colab import files\n",
        "#files.upload()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHbddaLQUNwo"
      },
      "source": [
        "# get file for given year from TfL open data\n",
        "!wget https://cycling.data.tfl.gov.uk/usage-stats/cyclehireusagestats-2014.zip\n",
        "!unzip cyclehireusagestats-2014.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# at this point we have Spark initialised and we have a number of CSV files.\n",
        "# NB you can try also download the zipfile to your host machine and try opening in Excel (Win)\n",
        "# (in Linux, easiest to open a file manager GUI then double-click on .csv file to open associated spreadsheet app)"
      ],
      "metadata": {
        "id": "wgH7CS0VBLPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmIQqnIFGKKl"
      },
      "source": [
        "# read in file\n",
        "!ls\n",
        "file=\"./1. Journey*csv\"\n",
        "file_1=\"./10a. Journey*csv\"\n",
        "files = os.listdir(\"./\")\n",
        "csv_files = [file for file in files if file.endswith(\".csv\")]\n",
        "spark = SparkSession.builder.appName(\"bikes\").getOrCreate()\n",
        "j_df = (spark.read.format(\"csv\")\n",
        "         .option(\"header\", \"true\")\n",
        "         .option(\"inferSchema\", \"true\")\n",
        "         .load(csv_files))\n",
        "\n",
        "# show top 10\n",
        "j_df.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data cleaning"
      ],
      "metadata": {
        "id": "4YiWqpBxencG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "j_df.columns"
      ],
      "metadata": {
        "id": "dIRdgWf7esBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "j_df.select(\"Duration\").filter(col(\"Duration\").isNull()).count()"
      ],
      "metadata": {
        "id": "dp6y4vxifCmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df = j_df.dropna(subset=[\"Duration\"])\n",
        "cleaned_df.select(\"Duration\").filter(col(\"Duration\").isNull()).count()"
      ],
      "metadata": {
        "id": "2hWHJA18f2LT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "j_df.select([count(when(isnull(c), c)).alias(c) for c in j_df.columns]).show()"
      ],
      "metadata": {
        "id": "Hx3-3_2EgTSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "j_df = j_df.dropna()\n",
        "j_df"
      ],
      "metadata": {
        "id": "LO05bNh3krTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "j_df.select([count(when(isnull(c), c)).alias(c) for c in j_df.columns]).show()"
      ],
      "metadata": {
        "id": "39FuQvqQk8Ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Total duration and Baylis Road duration total"
      ],
      "metadata": {
        "id": "lfxaXfLhZAFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,sum, isnan, isnull, count, when\n",
        "j_df.filter(col(\"StartStation Name\") == \"Baylis Road, Waterloo\").count()"
      ],
      "metadata": {
        "id": "c46VyAAHRUmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total duration\n",
        "total_duration = j_df.select(\"Duration\").agg(sum(\"Duration\").alias(\"Duration_sum\"))\n",
        "total_duration.show()"
      ],
      "metadata": {
        "id": "frxyEiA7XCUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baylis_filter = j_df.filter(col(\"StartStation Name\") == \"Baylis Road, Waterloo\")\n",
        "\n",
        "baylis_filter.agg(sum(\"Duration\").alias(\"Duration_sum\")).show()\n"
      ],
      "metadata": {
        "id": "CA_tIlSAXX5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aggregation (Group by Station name and order by Duration)\n"
      ],
      "metadata": {
        "id": "1BDvUqQ8ZuBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "j_df.printSchema()\n",
        "# j_df.groupby(\"StartStation Name\").min(\"Duration\").show()"
      ],
      "metadata": {
        "id": "8MKb1mxmZ8FK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "j_df = j_df.withColumn(\"Duration\", col(\"Duration\").cast(\"int\"))"
      ],
      "metadata": {
        "id": "5qH7zNgzbRiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "j_df.groupby(\"StartStation Name\").sum(\"Duration\").orderBy(\"sum(Duration)\", ascending=True)\\\n",
        ".filter(col(\"StartStation Name\") ==  \"Baylis Road, Waterloo\").show()\n"
      ],
      "metadata": {
        "id": "VjBOGfXTbeeT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}